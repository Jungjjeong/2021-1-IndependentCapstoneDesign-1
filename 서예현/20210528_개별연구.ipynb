{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "10.Word2Vec_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es1Y08au5xLq"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minsuk-heo/tf2/blob/master/jupyter_notebooks/10.Word2Vec_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRgUlodq5xLt"
      },
      "source": [
        "# Sentence Classification using LSTM and Pretrained Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDlT184Q5xMG"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kfl0H3T5xMG"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN7-EwaI5xMH"
      },
      "source": [
        "# Load Pretrained Word2Vec\n",
        "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250/2\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72mcxHH55xMH"
      },
      "source": [
        "def get_max_length(df):\n",
        "    \"\"\"\n",
        "    get max token counts from train data, \n",
        "    so we use this number as fixed length input to RNN cell\n",
        "    \"\"\"\n",
        "    max_length = 0\n",
        "    for row in df['review']:\n",
        "        if len(row.split(\" \")) > max_length:\n",
        "            max_length = len(row.split(\" \"))\n",
        "    return max_length\n",
        "\n",
        "def get_word2vec_enc(reviews):\n",
        "    \"\"\"\n",
        "    get word2vec value for each word in sentence.\n",
        "    concatenate word in numpy array, so we can use it as RNN input\n",
        "    \"\"\"\n",
        "    encoded_reviews = []\n",
        "    for review in reviews:\n",
        "        tokens = review.split(\" \")\n",
        "        word2vec_embedding = embed(tokens)\n",
        "        encoded_reviews.append(word2vec_embedding)\n",
        "    return encoded_reviews\n",
        "        \n",
        "def get_padded_encoded_reviews(encoded_reviews):\n",
        "    \"\"\"\n",
        "    for short sentences, we prepend zero padding so all input to RNN has same length\n",
        "    \"\"\"\n",
        "    padded_reviews_encoding = []\n",
        "    for enc_review in encoded_reviews:\n",
        "        zero_padding_cnt = max_length - enc_review.shape[0]\n",
        "        pad = np.zeros((1, 250))\n",
        "        for i in range(zero_padding_cnt):\n",
        "            enc_review = np.concatenate((pad, enc_review), axis=0)\n",
        "        padded_reviews_encoding.append(enc_review)\n",
        "    return padded_reviews_encoding\n",
        "\n",
        "def answer_encode(answer):\n",
        "    \"\"\"\n",
        "    return one hot encoding for Y value\n",
        "    \"\"\"\n",
        "    if answer == 'positive':\n",
        "        return [1,0]\n",
        "    else:\n",
        "        return [0,1]\n",
        "    \n",
        "def preprocess(df):\n",
        "    \"\"\"\n",
        "    encode text value to numeric value\n",
        "    \"\"\"\n",
        "    # encode words into word2vec\n",
        "    reviews = df['review'].tolist()\n",
        "    \n",
        "    encoded_reviews = get_word2vec_enc(reviews)\n",
        "    padded_encoded_reviews = get_padded_encoded_reviews(encoded_reviews)\n",
        "    # encoded answer\n",
        "    answers = df['answer'].tolist()\n",
        "    encoded_answer = [answer_encode(answer) for answer in answers]\n",
        "    X = np.array(padded_encoded_reviews)\n",
        "    Y = np.array(encoded_answer)\n",
        "    return X, Y "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osd7_vgU5xMJ"
      },
      "source": [
        "# Preprocess (encode text to number)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ZFoe5M5xMJ"
      },
      "source": [
        "movie_reviews_train = [\n",
        "         {'review': 'Andy ate an apple',                          'answer': 'positive'},\n",
        "         {'review': 'Jack was shocked by his test score',         'answer': 'negative'},\n",
        "         {'review': 'David hung out with his friends',            'answer': 'negative'},\n",
        "         {'review': 'Professor Kim gave students a present.',     'answer': 'positive'}\n",
        "    ]\n",
        "df = pd.DataFrame(movie_reviews_train)\n",
        "\n",
        "# max_length is used for max sequence of input\n",
        "max_length = get_max_length(df)\n",
        "\n",
        "train_X, train_Y = preprocess(df)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n17JLjI5xMJ"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beajf86C5xMM"
      },
      "source": [
        "# LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV4-kNXZ5xMN"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psSgL7tB5xMR"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ditGUgRe5xMR",
        "outputId": "5195331a-33ad-4e3e-a4d5-8fee95919c82"
      },
      "source": [
        "print('Train...')\n",
        "model.fit(train_X, train_Y,epochs=50)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7065 - accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6946 - accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6833 - accuracy: 0.7500\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6724 - accuracy: 0.7500\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6617 - accuracy: 0.7500\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6511 - accuracy: 0.7500\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6404 - accuracy: 0.7500\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6297 - accuracy: 0.7500\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6188 - accuracy: 0.7500\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6078 - accuracy: 0.7500\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5965 - accuracy: 0.7500\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5849 - accuracy: 0.7500\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5730 - accuracy: 0.7500\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5609 - accuracy: 0.7500\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5484 - accuracy: 0.7500\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5355 - accuracy: 0.7500\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5223 - accuracy: 0.7500\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5087 - accuracy: 0.7500\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4948 - accuracy: 0.7500\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4804 - accuracy: 0.7500\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4657 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4506 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4352 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4193 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4031 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3865 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3695 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3522 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3346 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3167 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2986 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2805 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2623 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2444 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2268 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2096 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1930 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1772 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1622 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1482 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1350 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1229 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1116 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1014 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0920 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0834 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0756 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0686 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0622 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0565 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb5f800ab50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utbEdZ0E5xMS",
        "outputId": "b096f1ed-591c-4b38-c3fa-eae75542a173"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, 32)                36224     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 36,290\n",
            "Trainable params: 36,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx-p54al5xMS"
      },
      "source": [
        "# Test\n",
        "your model can predict correctly even for unseen words from training.  \n",
        "This is the most benefit of using pretrained word embedding.  \n",
        "Why? pretrained embedding will encode [better], [nice] to similar vector of [best]  \n",
        "even if these words were not in train.  \n",
        "therefore, the input vector to RNN is similar, so correct answers for even these unseen words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3f7_a6B5xMT",
        "outputId": "689f3a5c-63c9-4c9d-edaf-8e27b2202640"
      },
      "source": [
        "\"\"\"\n",
        "movie_reviews_train = [\n",
        "         {'review': 'Andy ate an apple',                          'answer': 'positive'},\n",
        "         {'review': 'Jack was shocked by his test score',         'answer': 'negative'},\n",
        "         {'review': 'David hung out with his friends',            'answer': 'negative'},\n",
        "         {'review': 'Professor Kim gave students a present.',     'answer': 'positive'}\n",
        "    ]\n",
        "\"\"\"\n",
        "movie_reviews_test = [\n",
        "         {'review': 'Andy did not ate an apple',                  'answer': 'negative'},\n",
        "         {'review': 'Jack was happy with his test score',         'answer': 'positive'},\n",
        "         {'review': 'David hang out with his friends',            'answer': 'negative'},\n",
        "         {'review': 'Professor Kim gave students a present',      'answer': 'positive'},\n",
        "         {'review': 'Professor gave students a reward',           'answer': 'positive'}\n",
        "    ]\n",
        "test_df = pd.DataFrame(movie_reviews_test)\n",
        "# print(test_df)\n",
        "\n",
        "test_X, test_Y = preprocess(test_df)\n",
        "# print(test_X)\n",
        "# print(test_Y)\n",
        "\n",
        "score, acc = model.evaluate(test_X, test_Y, verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 - 0s - loss: 1.1236 - accuracy: 0.6000\n",
            "Test score: 1.1236151456832886\n",
            "Test accuracy: 0.6000000238418579\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}